---
title: "Hadoop file system (HDFS)"
author: "Arthur katossky & Rémi Pépin"
date: "18/02/2021"
output: 
  xaringan::moon_reader:
    nature:
      highlightLines: 
      ratio: 16:10
      scroll: false
    css: css/xaringan-themer.css

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Spark in a nutshell

- A widely use computing engine
- *"In memory MapReduce"*
- Python and R friendly
- Can process *very large file* 
- Can run without HDFS or the Hadoop ecosystem


---
## The Hadoop Ecosystem

<img src="https://static.packt-cdn.com/products/9781788995092/graphics/c8625da0-2ffb-41b7-bba8-58c33af68a30.png" style="width: 80%; display: block; margin-left: auto;margin-right: auto" />

---

## The HDFS architecture

### Architecture

Main/workers architecture<sup>1</sup>

- **Driver** (main): maintains information about the Spark Application, responding to user's programme, analyse, distributes and schedules works. Can be your own computer in *client-mode* or a server in *cluster-mode*
- **Executors** (worker): workhorse of the system. Keep data in memory and do the computation. If on top of HDFS, executors and datanode are running on the same machine. 

.footnote[1:The real architecture is more complex because of the Resource Management Layer]

---

## Key Concepts

### In memory data

- Spark keeps data in memory (RAM) (like R or Python)
- Spark is a distributed system

--

So your data are not on only one machine ! There are spred among your cluster, **WITHOUT REPLICATION**



---
class: center, middle


<img src="https://media.giphy.com/media/EdRgVzb2X3iJW/source.gif" style="width: 80%; display: block; margin-left: auto;margin-right: auto" />

---
## Key Concepts

### Transformations and Actions

- Spark has two types of methods :
  - **Transformation** : takes a dataframe in input and produces another dataframe
  - **Action** : takes a dataframe in input and writes it to output data sources (console, file)
  

--
- You just chained **transformations** to design your process. And you use an action to un it


---

## Key Concepts

### Transformations

Commons transformation are :

- ` map()`/`flatmap()` : applies a function to each row

- `filter()` : filters row ? (¯\_(ツ)_/¯)

- `select()` : select columns

- `selectExpr()` : To run non aggregating SQL statement (the select part -> compute new columns)


---

### How does it work ?

![](img/hdsf/hadoop example.png)

---

### How to get a file ?

- A client requests a file to the NameNode 
- The NameNode selects for each file block a DataNode
- It sends back to the client where to find each block
- The client request the DataNode

--

**Pros**: 

- No network nor process overload for the NameNode
- The NameNode can handle a lot of request at the same time 
- Try to select the best DataNode

--

**Con**:

- NameNode single point of failure (there is solution)

---
## How to use HDFS ?

- You need a cluster with HDFS to store file (obviously) 
- HDFS doesn't provide a fancy GUI to interact with  :
  - Install the hadoop CLI on your machine
  - Use the Java API 
  - Use the [WebHDFS REST API](https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/WebHDFS.html)
- Install HUE, an GUI for the Hadoop ecosystem


---
## Pros of HDFS

- Fault tolerance :
  - Resilient to DataNode failure
  - NameNode failure can stop the system for some time, but if the log and the namespace image are still here the system can reboot (that's why they are persist on disk)
- High throughput
- Scale easily
  - Just add mote DataNode for more storage capacity
  - For very large system one NameNode can be insufficient 

---

## Cons of HDFS

- High latency data access
- Can't handle lots of small file
- No concurrent write for the same file
- No arbitrary modification of a file. Can only add content at the end

---
## When HDFS is a good choice ?

- To store very big files
- To store read only files
- To build all the Hadoop stack on top
- To store file for batch processing

---
## When HDFS is a bad choice ?

- To store a lots of small files
- To store files that are edited a lot
- To store file used by system which need low latency

---
## Keep in mind

- HDFS is only a storage solution !
- There is a framework to process the data called **MapReduce**
- HDFS doesn't have a GUI, it's all command line (CLI)
- [HUe](https://gethue.com/) can provide a nice GUI