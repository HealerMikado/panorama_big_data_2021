---
title: "Spark"
author: "Arthur katossky & Rémi Pépin"
date: "18/02/2021"
output: 
  xaringan::moon_reader:
    nature:
      highlightLines: 
      ratio: 16:10
      scroll: false
    css: css/xaringan-themer.css

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Spark in a nutshell

- A widely use computing engine
- *"In memory MapReduce"*
- Python and R friendly
- Can process *very large file* 
- Can run without HDFS or the Hadoop ecosystem


---
## The Hadoop Ecosystem

<img src="https://static.packt-cdn.com/products/9781788995092/graphics/c8625da0-2ffb-41b7-bba8-58c33af68a30.png" style="width: 80%; display: block; margin-left: auto;margin-right: auto" />

---

## Common Spark APIs

--
- Scala (the default one)

--
- Java

--
- **Python** ([pyspark](https://spark.apache.org/docs/latest/api/python/))

--
- **R** ([SparkR](https://spark.apache.org/docs/latest/sparkr.html), [sparklyR](https://spark.rstudio.com/))

--
- **SQL** ([Spark SQL](https://spark.apache.org/sql/))

---

## The Spark architecture

### Architecture

Main/workers architecture<sup>1</sup>

- **Driver** (main): maintains information about the Spark Application,
responding to user's program, analyze, distributes and schedules works.
Can be your own computer in *client-mode* or a server in *cluster-mode*.
If the driver fails, everything fail
- **Executors** (worker): workhorse of the system. Keep data in memory and 
do the computation. If on top of HDFS, executors and datanode are running on 
the same machine. If an executors fail, just need some recomputation

.footnote[1:The real architecture is more complex because of the Resource Management Layer]

---

## Key Concepts

### In memory data

- Spark keeps data in memory (RAM) (like R or Python)
- Spark is a distributed system

--

So your data are not in only one machine ! They are spread among your cluster, **WITHOUT REPLICATION**

???


---


## Key Concepts

### Dataframe

- Spark manipulates **tabular data** :

--

  - Fixed number of typed columns (schema)
  - Every line must have a value for every column (`null` is ok)

--
- A DF is spread among your cluster

--
- You manipulate the DF like it's only in one machine

--
- **A DF is immutable (you can't modify it once it's created)**

---



class: center, middle

<img src="https://media.giphy.com/media/EdRgVzb2X3iJW/source.gif" style="width: 80%; display: block; margin-left: auto;margin-right: auto" />

---

## Key Concepts

### Transformations and Actions

- Spark has two types of methods :

  - **Transformation** : takes a dataframe as input and produces another dataframe as output
  - **Action** : takes a dataframe as input and writes it to output data sources (console, file)
  
  

---
## Key Concepts

### Lazy evaluation

Wait the very last moment to process computation. You manipulated "hollow" dataframe

--
Writing a **transformation** don't run any computation.

--

You chained **transformations** to design your process. And you use an **action** to run it.

---

## In a nutshell

<img src="img/meme/plan to dataframe.jpg" style="height: 80%; display: block; margin-left: auto;margin-right: auto" />


---

## Key Concepts

### Transformations

Some common transformations :

- `df.map()`/`flatmap()` : applies a function to each row
- `df.filter()` : filter rows 
- `df.select()` : select columns
- `df.selectExpr()` : run `SELECT` SQL statement 
- `df.union(otherDataframe)`: return a dataframe with the union of the element of the source and the argument
- `df.distinct()` : Return a new dataframe that contains the distinct elements of the source dataframe
- `df.groupby()` : group a DF by a criteria
-  `df.sum()`/`avg()`/`mean()` : if grouped DF return one line by group

???
- map : one row = one row
- flatmap : one row = multiple row
---

class: center, middle
<img src="http://www.quickmeme.com/img/71/71d4f9e6c603698a186bff6dd8ea8c3fd1b5ad9c9fb37455b8d114b1938a735b.jpg" style="height: 80%; display: block; margin-left: auto;margin-right: auto" />

---
## Key Concepts

### `sql()` : one transformation to rule then all<sup>1</sup>

Spark understands sql statements !

--

```SQL
spark.sql("""
	SELECT user_id, deartement, first_name
	FROM professors
    WHERE departement IN
    	(SELECT name FROM departement WHERE created_date >= '2016-01-01')
	""")
```


.footnote[
1 : not all it's just for the dramatic effect
]


---
## Key Concepts

### `sql()` : one transformation to rule then all

Pros :

- Easy to understand
- Return a DF so you can chain it with another transformation
- One of the most powerful feature of Spark

--

Cons :

- Some very complex transformations are harder do to in SQL than in Spark


---

## Key Concepts

### Actions

Trigger computation. So they take time !

--

Common actions :

- `df.show(n=20)` : print the first n row
- `df.count()`: return the number of line in the dataset
- `df.take(n)`: return the first n rows as `list` of `Row`
- `df.write.format().mode().save()` : write the DF to file

---
## What if spark wasn't lazy ?

- Each line of code could take forever

--
- User can be dumb (*filter data at the end*)

--
- Each transformation produce a DF. 10 transformations = 10 DF

--

Only user based optimization


---
## Knowledge is power

--
- Optimizes your transformations chain because it knows all your transformations
(*execution plan*)

--
- Does not keep useless data in memory (better use of memory)



???
Postgres has one too

We are in a big data situation, and we do not want to keep in memory
useless data.

---
## Lazy excecution : sum up

--
- Pros :
  - Spark optimizes your operations
  - Data are not kept in memory

--
- Cons :
  - Cannot reuse the same DF

---
## Caching DF

Caching make it possible to store in memory or disk a DF for future reuses.

--
-  `df_to_cached.cache()` to cache a DF

--
- It's a lazy operation (cached only when computed)

--
- Very useful in a **interactive data science session** ;) ;)

