---
title: | 
    | Introduction to Big Data
    | Lesson 1.4 What if ... ?
author: "Arthur Katossky & Rémi Pépin"
date: "Friday, March 12, 2021"
output: 
  xaringan::moon_reader:
    nature:
      highlightLines: 
      ratio: 16:10
      scroll: false
      countIncrementalSlides: false
    css: ["css/xaringan-themer.css", "css/mine.css"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## What if ... ?

???

We have to distinguish between estimation of a statistical model (always the same call, happen seldom, often only once or twice), developement/research (many requests, often very different from each other) and model in production, for instance for prediction (usually lighter computations, but happen possibly very often). It is rare to be in the case where you both want a very intense computation and you wan to perform it often, exceptions being for instance 3D rendering.

---

## What if data is too big to fit in memory ?

.pull-right[![](https://image.flaticon.com/icons/png/512/906/906794.png)]

--
1. do less (filter, select, sample)

--
2. buy more/bigger memory

--
3. do things in chunks / stream

--
4. distribute the data

---

## What if ... ?

---

## What if your file is too big for your local file system ?

.pull-right[![](https://image.flaticon.com/icons/png/512/906/906794.png)]

--
1. update file system

--
2. import less data (filter, select, sample)

--
3. cut file into pieces and process in chunks<br><small>for instance 1 file per day</small>

--
4. if you file is slight structured, go onto databases, that's what they are made for

--
5. rent a remote cloud-based service, they will abstract the problem away from you

???

Sometimes you don't have the choice. You get data you have to read before you can filter.

---

## What if your file is too big for your local file system ?

![](img/cloud/limit-size-aws.png)

---

## What if your file is too big for your local file system ?

![](img/cloud/limit-size-azure.png)

---

## What if your file is too big for your local file system ?

![](img/cloud/limit-size-gcp.png)

---

## What if ... ?

---

## What if data is too big to fit on disk ?

.pull-right[![](https://image.flaticon.com/icons/png/512/906/906794.png)]

--
1. store less (filter, select, sample)

--
2. buy bigger physical disk

--
3. buy your own storage server

--
4. rent storage space (file space or database) in the cloud

--
5. distribute data yourself on several nodes in a cluster

???

Database may be helpful, but won't solve the issue of not enough space on the disk.

---

## What if ... ?

---

## What if computation takes ages ?


.pull-right[![](https://image.flaticon.com/icons/png/512/906/906794.png)]

--
1. do less (filter, select, sample), especially in development stages

--
2. do less (less tests, less formatting)

--
3. go low-level (compile, use C or C++, use command shells ... or build chips!)

--
4. profile your code (how does it scale ? where are the bottlenecks?)

--
5. buy bigger or more cores

--
6. rent cores on the cloud

--
7. avoid i/o or network operations

--
8. **parallelise on non-local nodes**

]

---

## What if ... ?

---

## What if computation / storage is too expensive ?

.pull-right[![](https://image.flaticon.com/icons/png/512/906/906794.png)]

--
1. store or compute less (filter, select, sample)

--
2. consider cloud computing<br><small>Not always cheaper, though!</small>

--
3. be careful with databases requests

--
4. go from RAM (expensive) to disk (cheap)

--
5. use smaller but many computing units (ex: scientific grids)

???

4. stream, caveat: more communication between disk and memory

---

## What if ... ?

---

## What if data i/o is too fast ?

.pull-right[![](https://image.flaticon.com/icons/png/512/906/906794.png)]

--
1. for computing: pipeline (overlapping chunk processing), stream (never-ending one-by-one processing)

--
2. for storage: fast databases

---

## What if ... ?

---

## What if data i/o is too varied ?

.pull-right[![](https://image.flaticon.com/icons/png/512/906/906794.png)]

--
1. dedicated file systems and databases

This is ultimately not a computation issue.