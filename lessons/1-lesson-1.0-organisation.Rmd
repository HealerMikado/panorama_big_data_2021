---
title: | 
    | Introduction to Big Data
    | Lesson 1.0 Course Introduction
author: "Arthur Katossky & Rémi Pépin"
date: "Wednesday, March 30, 2022"
output: 
  xaringan::moon_reader:
    nature:
      highlightLines: 
      ratio: 16:10
      scroll: false
      countIncrementalSlides: false
    css: ["css/xaringan-themer.css", "css/mine.css"]
---

# An intoduction to something _big_ 

Two revolutions at the same time

- Big data revolution : data science oriented 
- Cloud computing : IT oriented


--
This course is a __introduction__ to this two revolutions. So you won't be experts
on those fields. But you will learn a lot about computers and IT in general

???
Because Big Data and Cloud Computing are related at some point, this course can
be technically hard. You will use multiples technologies you still don't know, but
they are important for a real data scientist

---

## The two speakers

- Rémi Pepin :  teaching assistants in the IT department and former Java developer at Insee
- Arhur Katossky : NLP enthusiast, currently student at Ensae, former  teaching assistants in the economy department.

---

## The objectives

1. Understand the basics of computation in the real world, the bottlenecks and how to solve them
2. Understand the basics of cloud computing and how to use AWS
3. Get familiar with big data technologies and the most common paradigm
4. Learn how to use Spark for data exploration on data at rest or streamed data, and how to some basics ML algorithm on big data

---

## How you will be graded

- One graded lab at the end of the coures
- One quick exam after the course
---

## Last warning

The labs are more like tutorials than practical session. Because you will discover
new things, labs follow the "To do X you have to use Y" paradigm.

--

If you think it's too easy, you can explore things by yourself =)

???
We think it's a good way to learn new things, because you can be autonomous and you are never blocked. If you think
it's too easy, you can explore things by yourself =). 

--- 

## A little game : which is a fastest ? 

- US weather data : 1 file per year, 71 years of data, around 6Go when gziped
- Extract the max temperature of each year
- 6 contestants :
  - a classic python loop (loop through file + read each line one by one)
  - a same but with Cython (python compiled in C)
  - the same code but in C. This code is compiled to machine code
  - The same code but in java. This code is compiled to byte code (need JVM)
  - python but each file are process in parallel (with 12 cores)
  - a `awk` command in bash to read each line of each file 



