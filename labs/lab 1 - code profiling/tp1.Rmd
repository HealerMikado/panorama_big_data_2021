---
title: "From Big Data to High-performance computing"
author: "Arthur Katossky & R√©mi P√©pin"
date: "April 2021"
output: 
subtitle: Lab 1 ‚Äî Profiling
institute: ENSAI (Rennes, France)
---

## A. Introduction

In this tutorial we are going to explore some common techniques to improve computing efficiency on a personal computer with R. A lot of this techniques do apply in other settings. We are going to use mostly **the exemple of the linear regression**, for several reasons:

1. you know the statistics behind it quite well
2. you will use it a lot in your professional life
3. it is slow enough on big data sets that the tricks we devise here will come handy in the future
4. it is fast enough that we don't lose hours in computation

When it comes to high-performance computation, there are typically four bottlenecks for efficient data processing on a personal computer:

- **processing**
- **memory**
- **storage**
- **networking**

This tutorial mainly deals with **processing** and **memory**. Networking inside a personal computer is often not a limiting factor (except maybe between CPU and GPU or between disk and memory), and issues rather rise from data transfer over long distances. Similarly, disk space is usually not an issue and when it is, personal computers are not the relevant tool anymore.

**IMPORTANT WARNINGS:**

- **some questions use VERY BIG data sets!** If your computer can not cope with any of the following exercises, just scale down the exercise on a smaller data set until your computer can in a reasonable amount of time.
- **at this level of technicality, things can escalate in complexity quite quickly!** You will often read that something "is beyond the scope of this tutorial" and that is completely normal. We are discovering knew tools.

## B. Know your machine

Since each of the potential bottlenecks depend on your specific hardware, it is so crucial to know the characteristics of your own computer. You will never encounter exactly the same issues than your fellow students! A fast computation on some computers can be quite slow on an other.

**Q1.** How many CPUs or cores does your computer have? Do you have a programmable GPU? <i><small>**On Windows**, open the task manager and go to the performance tab. On Windows 10, the number of processors is directly written while on earlier versions, you have to count the number of graphs in the upper pannel. **On Linux systems**, click on the gear menu, then "About this computer". Alternatively, open a terminal and type `lscpu`. **On Mac OS**, click on the Apple menu, then "About this Mac". Alternatively, open a terminal, type `sysctl -a` and look for `machdep.cpu.core_count`. **In RStudio Server and from R in general**, you can use `parallel::detecCores(logical=...)`, provided the package `parallel` is installed. (You can also execute command lines with the `system(...)` command, for instance `system("lscpu")` on the cluster, that runs on Linux.) Notice that on the cluster, `detectCores()` will detect the cores accessible to all users.</small></i>

‚öôÔ∏è Many computers will display "virtual" or "logical" cores, that is several virtual processors running simultaneously on the same "physical" core (i.e. the same physical chip, the same circuit). We will not consider this distinction in this tutorial, so feel free to count logical or physical cores.

**Q2.** How much memory do you have? Make sure you know the two conventions for converting bytes into kB and GB. <!-- using 1024 or 1000 --> <i><small>**On Windows, Mac OS or Linux** see before. (On a Mac terminal, search for `hw.memsize` in the output of `sysctl -a` and on a Linux terminal, type `free -h`.) **In RStudio Server and from R in general**, you can only use the `system(...)` command, that is, since at ENSAI the cluster runs on Linux, `system("free -h")`. Notice that on the cluster, this command will detect the memory accessible to all users. Please also note that the `free` command provides information about "usable" memory (as seen by the operating system), whereas the other commands list constructore information, potentially larger.</small></i>

**Q3.** How much available disk space do you have? _You probably already know how to do this! Just right click on your hard disk's icon or open the disk management utility._

**Q4.** How much data can you transfer out to and in from the Internet? _You can use any test website, such as https://www.speed.io to measure it._

üí° One of the reasons why to use **standardized machines** from **cloud computing providers** is to be able to use (virtually) the same machines than your colleagues and be able to control the computing better.

## C. Creating a massive data set

Let's create a big simulated data set, that we are going to use throughout the tutorial. (We use an artificial data set so that we do not lose too much time on pure data transfer, but please imagine it is a very precious data set you spent months collecting, and that is going to help you resolve most if not all of Humanity's problems.)

```{r, echo=T}
library(tibble) # better printing for data frames

# maximal number of rows (N) and columns (P)
N <- 100000
P <- 500

predictors <- tibble(rnorm(n = N))

for(p in 2:P){
  # each column depends on the previous one, creating a light form of colinearity
  predictors[, p] <- 0.5*predictors[, p-1] + 0.5*rnorm(n = N)
}

colnames(predictors) <- paste0("X", 1:P) # let's give nive column names: X1, X2, X3, etc.

predictand <- rowSums(predictors)+rnorm(N) # Y = X1+X2+X3+...+U
```

One can measure the size of the data set with `object.size()` or with the `obj_size()` function from the `lobstr` package (better printing, accepts several objects at once).

**Q5.** In R Studio, you can have a view of the size of the object in the "grid" view of the "environment" panel. Do you understand why the size seems to differ with `lobstr::obj_size()` ? <!-- R Studio has a 1KB = 1024 B policy : obj_size(predictors)/1024^2 -->

<small>‚öôÔ∏è `lobstr::obj_size()` just means that "the `obj_size()` function coming from the `lobstr` package". (You may also see `:::` instead of `::` for functions hidden inside a package and not accessible through the usual loading with `library()`.) In this tutorial, it is useful for explicitly telling you which package a function belongs to.</small>

We will later see (section XXXXXXXX) how to do when data can not stand in memory any more, but could fit on disk. Since R, contrary to SAS, loads the full data set in memory in order to process it, it needs special tools for bigger-than-memory data.

## D. Code profiling

The following bit of code runs a linear regression in R and takes quite some time on my machine. (Feel free to increase/decrease the size of `predictors` and `predictand` until it takes about ten to fourty seconds to process.)

```{r, echo=TRUE, eval=FALSE}
lm(predictand ~ ., data = predictors)
```

Anticipating a possible bottleneck, we may ask several questions about this code:

a. How fast is it depending on the input size?
b. How much memory does it use?

Storage and networking are not an issue for now, since in the worse case, this data set is still going to fit on the hard disk.

### D.a Measuring execution time

Among all the possible ways to **measure execution time** in R, I recommend using the `bench` package and specifically the `bench_time()` function[^a]. `bench_time()`, as most of time measuring functions, distinguishes between "processing time" (how long did you use the processor(s)) and "real time" (how long did it take between the first and the last instruction). The two may differ, for instance when the processor is just waiting for a new instruction, as demonstrated here:

```{r}
library(bench)
bench_time( Sys.sleep(1) ) # sleep for one second, no actual computation
```

[^a]: Other methods include:
    
    - the base-R `Sys.time()` function, which returns the time as evaluated by the operating system (you get the processing time by difference between two calls)
    - the `tic()` and `toc()` functions from the `tictoc` package, whose advantage is that you can nest several calls into one another, and thus measuring nested parts of your code ([here](https://collectivemedia.github.io/tictoc/tic.html) for more)
    - the `microbenchmark` package and the eponymous function ; the package is intended for very fast sub-functions, and the default option of 100 repetitions may be exaggerated for our use ; `microbenchmark()` has rudimentary plotting options

**Q6.** How long is taking the linear regression defined earlier? <!-- just a way to make student use bench_time(lm(predictand ~ ., data = predictors)) -->

**Q7.** Repeat the measurement a couple of times. Does each repetition have exactly the same duration? How can you explain it? <!-- There is concurrent use of the CPU with other programs on the computer and, ultimately, physical processes that can interfere with the computation. The computer is never in exactly the same state than earlier. -->

### D.b Measuring memory allocation

Following **memory use** is a harder task than time measurement. Indeed, it is not enough to compute the difference of space used before and after some expression has been executed, since temporary variables may have been created and deleted in between. The `bench` package is also helpful here: `bench_process_memory()` gives the current and maximum possible memory allocated by the system to R ; and `bench_memory(expression)` returns a data frame containing the total memory allocation (`mem_alloc`) and the detailed sequence of allocations (`memory`).

```{r, echo=TRUE}
library(bench)
library(lobstr)
bench_memory(vec <- c(runif(100), rnorm(3))) # > 6 kB
obj_size(vec)                                # < 1 kB
```

**Q8.** What is the maximal available memory from R on your computer? Does it makes sense when compared to your memory size? Where does the difference come from? <!-- bench_process_memory() ; The memory is used by all programs, notably the operating system... R can only access a portion of that.-->

**Q9.** How much memory does the `lm()` function consume? Is it in line with the size of the `lm()` object? Where does the difference come from? <!-- bench_memory(result <- lm(predictand ~ ., data = predictors)) ; obj_size(result) On my computer 470 MB (memory) vs. 175 MB (object) ; this comes from the fact that much more computation is performed under the hood, like matrix operations, that need a lot of space -->

```{r, echo=FALSE, eval=FALSE}
# correction

library(bench)
bench_memory( model_results <- lm(predictand ~ ., data = predictors) )

library(lobstr)
obj_size(model_results)
```

### D.c Comparing expressions with complete, advanced profiling

The `bench` package has a nice `bench::mark()` function that combines both approaches and offer a complete profiling of a given expression. It repeats a function up to `max_iterations` times if the function is fast (1000 times by default), but only `min_iterations` if it is slow (1 time by default). It can compare several expressions to verify that they are equivalent. Plus it has rudimentary but nice plotting features.

```{r, echo=TRUE}
library(bench)
benchmark_results <- bench::mark(
  # check = FALSE,        # if you don't want to check that your function do exactly the same thing
  # max_iterations = 100, # if you don't want to loose time on fast functions (defaults to 1000)
  # min_iterations = 5,   # if you want to force at least 5 evaluations
  buil_in_sum  = sum(1:100),
  explict_loop = {s <- 0 ; for(i in 1:100) s <- s+i ; s}
)
benchmark_results
plot(benchmark_results)
```

**Q10.** Repeat the profiling of `lm` using `bench::mark()` twice, once on the first 100 observations, once on the first 1000.

```{r, echo=FALSE, eval=FALSE}
# correction
library(bench)
benchmark_results <- bench::mark(
  check=FALSE, # important
  min_iterations = 5, # better
  first_100  = lm(predictand[1: 100] ~ ., data = predictors[1: 100, ]),
  first_1000 = lm(predictand[1:1000] ~ ., data = predictors[1:1000, ])
)
benchmark_results
plot(benchmark_results)
```
<small>‚öôÔ∏è You will sometimes get information about "garbage collection" or see information about "gc" or "gc level". When some variables get implicitly deleted in R, for instance when a function ends and all the local variables inside this function get discarded, R does not immediately recover the memory. Instead, it waits until it can get back a bigger chunk of memory, aka "garbade collection". Different levels correspond to how extreme R is in this behavior. Since this operation takes time, a run that triggers garbage collection takes longer than usual. That's why the `bench` package mentions it.</small>

## E. The "problem size" : theoretical and empirical complexity

We know how long and how much memory it takes for running the code for $n =$ `r format(N, scientific = FALSE, big.mark = " ")` and $p=$ `r format(P, scientific = FALSE, big.mark = " ")` but how is it going to scale if we get 1 000 000 or even 10 000 000 rows ? Or if we get 100 more predictors? In order to know, we have to understand **how processing time and memory requirements vary according to the "problem size"**, in our case the number of observations $N$ and the number of predictors $P$. We can have either an empirical approach (measuring the change) or a theoretical approach (investigating the algorithm), and both are useful.

**Q11.** Can you imagine contexts where collecting more data would mean increasing $n$? Increasing $p$? Increasing both $n$ and $p$? <!-- increase in $n$ almost any data recording ; increase in p alone or in p AND n : medical / biological research, where more money can be used either for collecting more samples  or for making more thorough tests or both -->

### E.a Empirical behaviour of with problem size

The empirical approach consists in measuring how the processing and the memory vary as a function of the problem size, here as a function of $n$ and $p$. We just need to try different values ! Extrapolating the observed behavior, we can hopefully determine some upper limit at which our own computer will fail.

Thanks to the `press()` function from the `bench` package, we can use `bench::mark()` while exploring a grid of values. A nice feature is that only the part in `bench::mark()` is profiled, so that we can perform preparation treatment that are not evaluated.

**Q12.** Execute the following code and make sure you understand all the steps while it is running.

```{r, eval=FALSE, echo=TRUE}
library(bench)

steps <- c(1,2,5) %o% 10^(1:floor(log10(max(N,P)))) # print steps

benchmark_results <- press(
  
  n = steps[100 <= steps & steps<=N],
  p = steps[ 10 <= steps & steps<=P],
  
  {
    data <- predictors[1:n, 1:p] # this is not evaluated
    y    <- predictand[1:n]
    bench::mark("lm" = lm(y ~ ., data = data))
  }
)

# # we store the root mean squared-error for later
# benchmark_results$rmse <- lapply(
#   benchmark_results$result,
#   function(result) sqrt(mean(result$residuals^2))
# )
# 

# we discard the results, since it takes a LOT of space
# (by default bench::mark() stores all the results)
benchmark_results$result <- NULL

if(! dir.exists("data")) dir.create("data")
save(benchmark_results, file="data/benchmark_results.RData")
```

**Q13.** Execute the following code to illustrate the performance of your algorithm, i.e. time and memory as a function of $n$ and $p$. What relationship between them does it suggest? Logarithmic? Linear? Polynomial? Exponential? Factorial? _(The gray lines represent linear growth. On the $n$-graphs, each line represents a different $p$ and conversely each line represents a different $n$ on the $p$-graphs. On the lower graph, blue lines represent constant $p$, whereas red lines represent constant $n$.)_ <!-- XXXXXXXXXXX -->

```{r, eval=TRUE, echo=TRUE}
library(ggplot2)
library(patchwork)
library(forcats)
library(dplyr)

load("data/benchmark_results.RData")

p1 <- ggplot(benchmark_results, aes(x=n, y=median)) +
  geom_line(aes(color=fct_rev(ordered(p)), group=p)) +
  geom_text(
    aes(label=scales::number(p)), nudge_x = -0.05, hjust=1, size=2,
    data = . %>% mutate(n=as.numeric(n)) %>% filter(n==min(n))
  ) +
  # geom_hline(yintercept = 60) + # 60 s
  scale_color_ordinal("Predictors (p)", direction=-1, guide="none")

p2 <- ggplot(benchmark_results, aes(x=p, y=median)) +
  geom_line(aes(color=fct_rev(ordered(n)), group=n)) +
  geom_text(
    aes(label=scales::number(n)), nudge_x = -0.05, hjust=1, size=2,
    data = . %>% mutate(p=as.numeric(p)) %>% filter(p==min(p))
  ) +
  scale_color_ordinal("Observations (n)", option="B", direction=-1, guide="none")

p3 <- ggplot(benchmark_results, aes(x=n, y=mem_alloc)) +
  geom_line(aes(color=fct_rev(ordered(p)), group=p)) +
  geom_text(
    aes(label=scales::number(p)), nudge_x = -0.05, hjust=1, size=2,
    data = . %>% mutate(n=as.numeric(n)) %>% filter(n==min(n))
  ) +
  # geom_hline(yintercept = 4*(2^10)^3) + # 4 Go
  scale_color_ordinal("Predictors (p)", direction=-1, guide="none")

p4 <- benchmark_results %>%
  ggplot(aes(x=p, y=mem_alloc)) +
  geom_line(aes(color=fct_rev(ordered(n)), group=n)) +
  geom_text(
    aes(label=scales::number(n)), nudge_x = -0.05, hjust=1, size=2,
    data = . %>% mutate(p=as.numeric(p)) %>% filter(p==min(p))
  ) +
  scale_color_ordinal("Observations (n)", option="B", direction=-1, guide="none")

( (p1 | p2) / (p3 | p4) &
    geom_abline(slope=1, intercept=10:-10, col="grey") &
    scale_x_log10(minor_breaks = 1:9 %o% 10^(1:4), expand = expansion(mult=c(0.15,0))))
```

<!-- improvement of the graphs (1) same colors in the two graphs [x] ; (2) room for extrapolation ; (3) algin x-axis ; (4) improve breaks on the y-axis ; (5) direct labelling [x] ; (6) better axis titling-->

```{r echo=TRUE, eval=TRUE}
library(ggplot2)
library(patchwork)

load("data/benchmark_results.RData")

p5 <- ggplot(benchmark_results, aes(x=n, y=p, z=median)) +
  geom_contour_filled(breaks=10^(-6:10)) +
  geom_text(
    aes(label=median),
    color="white", size=2,
    angle = 30
  ) +
  scale_x_log10() +
  scale_y_log10() +
  guides(fill="none")
  
p6 <- ggplot(benchmark_results, aes(x=n, y=p, z=mem_alloc)) +
  geom_contour_filled(breaks=10^(1:10)) +
  geom_text(aes(label=mem_alloc), color="white", size=2, angle=30) +
  scale_x_log10() +
  scale_y_log10() +
  guides(fill="none")

p7 <- ggplot(benchmark_results, aes(x=median, y=mem_alloc)) +
  # geom_hline(yintercept = 4*(2^10)^3) + # 4Go
  # geom_vline(xintercept = 60) + # 60s
  geom_line(aes(group=n), size=0.1, col="red") +
  geom_line(aes(group=p), size=0.1, col="blue")
  
(p5 | p6) / p7 + plot_layout(heights = c(1,3))
```
<!-- improvement: add n and p on the above graphic -->

**Q14.** Recall your personal computer memory limit. What is approximately the maximal $n$ you can tackle, with $p=10$? <!-- You can use either the top or the bottom graph and prolong the curves until they hit the line corresponding to mem_alloc=2 Go. Just uncomment the lines in the graph to draw the line. The importance is not the exact number but the fact that we can more or less estimate it. -->

**Q15.** Imagine you can't wait more than one minute for preliminary results. What is the maximum $n$ you can hope to reach with $p=100$ ? <!-- Just uncomment the line in the top-right graphic to draw the line corresponding to 1 min. You can prolong the line corresponding to p=100 to get an idea. The importance is not the exact number but the fact that we can more or less estimate it. -->

**Q16.** Imagine you can't wait more than 2 hours for definitive results. Give two (n,p) couples you can hope reaching. <!-- Even if you wait 2 hours, most of (n,p) couples are not reachable due to memory limits. -->

### E.b The linear regression algorithm

You may remember from class that the coefficients $\beta$ of a regression are best estimated following:

$$\hat{\beta} = (X'X)^{-1}X'Y$$

... where $X$ represents the predictors and a column of ones and $Y$ represents the predictand. We could na√Øvely use this approach for the actual computation **BUT** inversing the $X'X$ matrix has (very) bad numerical numerical properties. Namely, this direct approach is _unstable_, meaning that some small changes to the $X$ matrix can lead to dramatic changes to the $\hat\beta$ output. (The details of why this is so are beyond the scope of this tutorial.)

<!-- Demander √† Romaric des r√©f√©rences sur les diff√©rentes sources de bruit dans les algo: bruit des donn√©es ; bruit de la forme fonctionnelle (qui n'est pas exactement celle r√©elle) ; bruit des erreurs d'arrondis ; bruit de l'isntabilit√© num√©rique de l'algo, etc. -->

Instead, the most common algorithm is the so-called "QR decomposition"[^2], where we first decompose $X$ into a product of two matrices $X=QR$, where $Q$ is a $(n,p)$-orthogonal matrix and $R$ is an upper triangular square matrix of size $p$. Assuming we know both $Q$ and $R$, the computation of $\hat\beta$ becomes:

[^2]: Alternative (effective) algorithms to solve the least-squares problem include the SVD decomposition and others.

$$\hat\beta=((QR)'(QR))^{-1}(QR)'Y\\=(R'\underbrace{Q'Q}_{=I_p \text{ as Q is orthogonal}}R)^{-1}R'Q'Y\\=R^{-1}\underbrace{(R')^{-1}R'}_{=I_p}Q'Y\\=R^{-1}Q'Y\\\iff R\hat\beta=Q'Y$$

Inverting $R$ is fast (and stable) because it is small (assuming $p \ll n$) and upper-triangular. There are several algorithms to find the QR-decomposition[^hou] but most of them return $Q$ and $R$ in $\mathcal{O}(np^2)$-time and $\mathcal{O}(np^2)$-space complexity. The $Q'Y$ multiplication takes $\mathcal{O}(np)$ operations and $\mathcal{O}(n)$ space, and solving for $\beta$ takes a final $\mathcal{O}(p^3)$ and no additional space. <!-- v√©rifier -->

[^hou]: R uses the so-called Householder method [source](https://svn.r-project.org/R/trunk/src/appl/dqrdc2.f)

### E.c Theoretical complexity

**Q17.** What is the total algorithmic complexity of the algorithm? <!-- Memory: $\mathcal{O}(np^2)$ Space: $\mathcal{O}(np^2)$ ; memory O(np3) parce que p3 pour la r√©solution du dernier Rb=Q'y et O(np2) pour obtenir R et Q ; space O(np2) parce que c'est la taille des matrices Q (np) et R (p2)
-->

**Q18.** Does it align with the empirical complexity measured before? <!-- For n, yes. For p, not so. But probably the effect is almost invisble since $p \ll n$ and that the $n$ effect in memory is huge. -->

**Q19.** Are there any **embarrassingly parallel** steps that we could easily divide between cores of a single machine? Are there parts we could delegate to the GPU? <!-- Nothing blatantly parallel. The matrix computations seem like an easy task for GPUs. -->

## F. Going low-level

### F.a Know your programming language

Every programming language has its specificity, where some instructions are fast compared to others. The reasons for these specificities usually go quite deep in the language implementation. Two famously slow processes in R are :

1. explicit loops (always prefer assignation)
2. assignation with <- c()/cbind()/rbind() (always prefer []<-)

#### F.a.1 In R, avoid loops and use vectorized functions

As many languages meant to perform mathematical computation, R is built around the abstraction of arrays, a generalization of vectors and matrices that is lacking in many other languages such as C or Python. Most functions accept and return vectors as input, performing element-wise operations such as in `1:10+3`, and **these vectorized functions are much faster than explicit loops** such as `v <- 1:10 ; for(i in 1:10) v[i]<-v[i]+3`. Loops are also often more memory-intense.

**Q20.** Benchmark the following and conclude.

```{r, echo=TRUE, eval=FALSE}
n <- 1000
p <- 50
M <- matrix(rnorm(n*p), nrow=n, ncol=p)

bench::mark(
  elementwise = {v <- numeric(n) ; for(i in 1:n){ for(j in 1:p) v[i] <- v[i]+M[i,j] } ; v},
  rowwise = {v <- numeric(n) ; for(i in 1:n) v[i] <- sum(M[i,]) ; v},
  vectorised = rowSums(M)
)
```

#### F.a.2 Negociating new space takes time

There is always some cost associated with negotiating memory space with the operating system. So if you know in advance the space some object is going to take, typically a vector or a matrix, you'd better reserve this space in advance. The typical thing you want to avoid is the pattern `all_results <- c(all_results, new_result)` when you are building an object recursively. Use `all_results[i] <- new_result` instead.

The problem you run into is both memory allocation (because each time you run out of place, the system has to find a new place in the memory, big enough to fit the additional data) and speed (because once some place has been found, the computer needs to copy all the values in the new slot).

**Q21.** Benchmark the following and conclude.

```{r, echo=TRUE, eval=FALSE}
n <- 100
p <- length(letters)

bench::mark(
  
  "explicit loop + <- cbind()" = {
    M <- NULL        # No pre-allocation
    for(i in 1:n) M <- rbind(M, letters, deparse.level = 0)
                     # One column is added to the matrix at a time,
                     # but the old M matrix is completely REPLACED by 
                     # the new matrix. Every single value is copied to a new
                     # spot in memory.
    
    # (The deparse.level option prevents R to give row names,
    # so that the outputs stays strictly the same
    # as with the other methods.)
    
    M
  },
  
  "explicit loop + []<-" = {
    M <- matrix(nrow=n, ncol=p)    # Pre-allocate the matrix space (filled with NAs)
    for(i in 1:n) M[i,] <- letters # Replace lines with actual values.
    M
  },
  
  "vectorized" = matrix(letters, nrow=n, ncol=p, byrow = T)
  # letters is repeated over and over until the matrix is filled,
  # an (sometimes surprising) behavior known as "recycling".
)
```

### F.b Use less human-friendly, lower-level functions

A lot of the time spent by higher-level, user-friendly functions such as `lm()` is actually spent performing pre-processing: testing that the parameters are in the correct format, making conversions, printing messages, etc. `lm()` for instance knows how to convert a data frame of factors into a nice numeric matrix on which you can perform matrix algebra. `lm()` makes a lot of extra computation that you do not necessarily need: it does not just return the beta coefficients, but also the predictions for the predictand, the errors, etc.

But if you are repeating very often the same operation, you may want to skip all the testing and formatting! The only caveat is that you have to ensure that the inputs have the right format by design! (Giving you a nice human-friendly experience is precisely the role of high-level functions.) Many high-level functions have lower-level counterparts.

**Q22.** Type `lm` (without the `()`) in your console to access the code of the function (this works with all functions in R) and spot the function call where computation actually take place. Repeat the operation with the new function. Why does R need so many functions? <!-- lm() calls lm.fit() which itself calls the C function "C_Cdqrls". R does not "need" so many functions, but there is high-level function, for humans, and a low level function, for programming. The human-level functions accepts a wide range of objects and transforms them so that the low-level one can handle it. We often don't want to learn how to write the proper C code that Cdqrls understands, and we are happy that any data.frame or matrix or vector or formula etc. can be converted to right format without having to think about it. -->

The second function, `lm.fit()` actually calls a C function named `C_Cdqrls`, performing the QR decomposition we talked about earlier (`qrls` stands for **QR** decomposition for the **l**east-**s**quares problem). C is a very low-level and thus very efficient language. There exists an other function, `.lm.fit()`, that skips the tests of `lm.fit()` and only calls the C function.

**Q23.** Benchmark the following and conclude. <!-- Little gain in time, but huge gains in memory! Almost no difference between lm.fit and .lm.fit -->

```{r, echo=TRUE, eval=FALSE}
predictors_m <- as.matrix(predictors) # lm.fit and .lm.fit only understand matrices
bench::mark(
  min_time = 5,
  check = FALSE,
  "lm" = lm(predictand~., data=predictors),
  "lm.fit" = lm.fit(predictors_m, predictand),
  ".lm.fit" = .lm.fit(predictors_m, predictand)
)
```

### F.c Just-in-time bytecode compilation

Since R version 3.4, **functions get byte-compiled** automatically, possibly giving you non-negligible improvements in memory and processing. It is thus advised to **wrap any costly code into a function!**

**Compilation** is the process of translating a high-level language into a lower-level language, closer to what your computer's processor actually execute. On the go, compiler perform all kind of optimization, like suppressing variables that are created but never used, re-writing loops, etc. **Byte-code** is the lowest-level platform-independent code, in the sense that compiling byte-code to lower-lever code would require a detailed knowledge about the actual architecture of the computer where the code is executed.

**Q24.** Benchmark the following and conclude: <!-- no improvement for well-written, efficient code, but huge improvement (10x) in speed AND memory for poorly written code -->

```{r, echo=TRUE, eval=FALSE}
numbers <- runif(1000)

# a poorly written Euclidean distance function
d1 <- function(nbs) {s <- 0 ; for(i in seq_along(nbs)){ s <- s+nbs[i]^2 } ; return(sqrt(s))}

# a better version
d2 <- function(nbs) sqrt(sum(nbs^2))

bench::mark(
  "loop" = {s <- 0 ; for(i in seq_along(numbers)){ s <- s+numbers[i]^2 } ; sqrt(s)},
  "loop+function" = d1(nbs=numbers),
  "vectorized" = {sqrt(sum(numbers^2))},
  "vectorized+function" = d2(nbs=numbers),
)
```

## G. Memory management

### G.a Sparcity

In order to save memory or disk space, many objects can be cached or stored in a very dense format, taking advantage of **redundancy in their structure**. That's how JPG images (compressed) are lighter that their BITMAP equivalent (raw pixel-based images), advantageously using the fact that two neighboring pixels are often of the same shade. In R, a similar effect can be seen in the storing of the "factor" type. Instead of a vector of repeated strings of characters, a vector of integer is used, attached to a dictionary for associating each integer value with a corresponding character value.

```{r, echo=TRUE, eval=TRUE}
n <- 2^16
words_str <- sample( c("Electro", "Jazz", "Techno"), size=n, replace=T)
words_fct <- factor(words_str) # encodes 1 string of characters (8 B) into 1 integer (4 B)
lobstr::obj_size(words_str) / n
lobstr::obj_size(words_fct) / n
```

Some matrices can also be very efficiently compressed, namely **sparse matrices**, matrices that contain a lot of zeroes. Diagonal matrices are such matrices. When the share of zeroes is superior two 2/3 of the values, it starts to be more efficient to store the $(i,j)$-positions of the non-zero coefficients. Also, **sparse matrix operations** may be very fast, since so many entries are zero! This can lead to impressive improvements in memory and speed.

**Q25.** Benchmark the following and conclude: <!-- (i,j,value) takes less place than storing the actual array, as long as non zero values represent more than 2/3 of the matrix ; sparse matrix multiplication is much more efficient -->

```{r, echo=TRUE, eval=FALSE}
library(Matrix)

n <- 1000

sample_matrix_with_zeroes <- function(n, prop) matrix(
  ncol=n, nrow=n,
  sample(0:9, size=n^2, prob=c(prop, rep( (1-prop)/9 ,9)), replace=T)
)

m1 <- sample_matrix_with_zeroes(n, prop=0)
m2 <- sample_matrix_with_zeroes(n, prop=2/3)
m3 <- sample_matrix_with_zeroes(n, prop=9/10)
m4 <- sample_matrix_with_zeroes(n, prop=99/100)

m1b <- Matrix(m1, sparse = TRUE)
m2b <- Matrix(m2, sparse = TRUE)
m3b <- Matrix(m3, sparse = TRUE)
m4b <- Matrix(m4, sparse = TRUE)

lobstr::obj_size(m1)
lobstr::obj_size(m1b)

lobstr::obj_size(m2)
lobstr::obj_size(m2b)

lobstr::obj_size(m3)
lobstr::obj_size(m3b)

lobstr::obj_size(m4)
lobstr::obj_size(m4b)

bench::mark(
  check = FALSE,
  "dense" = m4 %*% m4,
  "sparse" = m4b %*% m4b
)
```

How is this even remotely related to linear regression? You actually may find some convoluted cases, for instance with censored data, where a big part of your predictors are zeroes. But by far the most common case is **linear regression on categorical variables**, where each categorical variable is expended to a set of mutually exclusive 1/0 variables.

**Q26.** Benchmark the following and conclude: <!-- not that much faster, but uses much less memory -->

```{r, echo=TRUE, eval=FALSE}
library(lobstr)
library(Matrix)
library(MatrixModels)

n <- 4*5*10000

data <- data.frame(
  appearance = rep(c("good","normal", "bad", "very bad"), times=n/4),
  stage      = rep(c("stage 1", "stage 2", "stage 3", "stage 4", "stage 5"), each=n/5)
)

data2 <- data.frame(
  appearance = rep(c("good","normal", "bad", "very bad"), times=n/4),
  stage      = rep(c("stage 1", "stage 2", "stage 3", "stage 4", "stage 5"), each=n/5),
  stringsAsFactors = TRUE
)

lobstr::obj_size(data)  # variables stored as strings
lobstr::obj_size(data2) # variables stored as factors

Y <- runif(n)
X  <- model.matrix(~ appearance + stage, data2)
X2 <- Matrix::sparse.model.matrix(~ appearance + stage, data2)

lobstr::obj_size(X)  # dense matrix
lobstr::obj_size(X2) # sparse matrix

bench::mark(
  check = FALSE,
  "dense" = lm.fit(X, Y),
  "sparse" = MatrixModels:::lm.fit.sparse(X2, Y)
)
```

<!-- There is also the SparseM package, that is possibly faster -->

### G.b Out of memory processing

When your data exceeds available memory size, you may want to **store it on disk**, and **process it by chunks**. A **chunk** is any set of partition of the original data. The process of splitting data in chunks has various almost-synonymous names such as **sharding** or **horizontal partitioning** (as opposed to vertical partitioning, where you store variables separately). The partition may or may not be organized per basis of one of the variables in the data set.

```{r, echo=T, eval=F}
library(ff)
# library(ffbase) # for some extra ff treatments
library(bench)

m <- 100   # number of batches / chunks
k <- 10000 # batch / chunk size
n <- m*k   # number of observations
p <- 10    # number of variables

normal_matrix <- matrix(numeric(n*p), ncol=p)
ff_matrix <- ff(vmode="double", dim=c(n,p), dimnames=list(NULL, LETTERS[1:p]))

obj_size(normal_matrix) # ~80 MB
obj_size(ff_matrix)     # ~ 5 kB only *in memory*

# fill with random variables
ffrowapply(X=ff_matrix, VERBOSE=TRUE, BATCHSIZE=k, {
  k_empricial <- (i2-i1+1)
  # i1 and i2 are the indices of the beginning and end of the chunk
  ff_matrix[i1:i2, ] <- matrix( rnorm(k_empricial*p), ncol=p)
})

# convert to data.frame
ff_dataframe <- as.ffdf(ff_matrix)

# some methods still work
dim(ff_matrix)
ff_matrix[1000,]
ff_matrix[,"A"]
ff_dataframe$G

# some don't
# sum(ff_matrix)
# rowSums(ff_matrix)
# sort(ff_dataframe)
```

**Q27.** Do you need this approach for storing a data set of 1 000 000 000 financial quotes, assuming there are only 3 columns, one for the stock quote (integer, 4B per value), one for the stock name (string, 8B per value) and one for the transaction time (date-time format, 8B per value) ? <!-- 1000000000*sum(c(4,8,8))/1024^3 >= 18 Go, so yes, it most probably exceed the RAM of any home computer -->

At least two other packages provide similar on-disk capabilities, the `bigmemory` and the `disk.frame` packages. Detailing how to use either `ff` or `bigmemory` is beyond the scope of this tutorial. Note that **you should NOT use this approach to store datasets definitively on your computer** and that **you should use databases instead**. This packages just help cope with the limited memory space available for statistical computation. Additionally, remember that writing and reading on disk is slow, so that circumventing the memory problem this way has the downside of slowing down your code.

**Q28.** In a nutshell, which computational limitation do these packages address? <!-- Allowing to write on disk solves the memory limitation, but not the processing limit, nor the storage limit (on the contrary, it uses space on the disk), nor the networking limit (on the contrary, frequent reads and writes on disk may use the network capacity). -->

## Further reading and sources

- Much recommended, Hadley Wickham's [**_Advanced R_**](https://adv-r.hadley.nz/index.html)
- David Goldberg, _What Every Computer Scientist Should Know About Floating-Point Arithmetic_ (1991)